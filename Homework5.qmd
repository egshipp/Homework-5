---
title: "Homework 5"
format: pdf
editor: visual
---

# Task 1: Conceptual Questions

-   What is the purpose of using cross-validation when fitting a random forest model?

    -   Cross-validation when fitting random forest models is not always necessary, but it is used to tune the random forest model by selecting the number of predictors that minimizes the log-loss metric.

-   Describe the bagged tree algorithm.

    -   The bagged tree algorithm is as follows:

    1.  Bootstrap sampling from an original data set
    2.  For each bootstrap sample, fit a tree model
    3.  Average the predictions for each tree model for regression or use majority vote across all the trees for classification.

-   What is meant by a general linear model?

    -   A general linear model is a statistical model used for prediction and inference that contains coefficients that are only linear by nature.

-   When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?

    -   An interaction term allows two predictors to be dependent on each other while a model without an interaction term assumes that the predictors are independent of each other.

-   Why do we split our data into a training and test set?

    -   Data is split into a training and test set so that the model can be "trained" on one set of data and "tested" on a set that is completely independent from that the data that the model was trained on. If the data was not split, then the test would not be indicative of how well the model is able to make predictions.

# Task 2: Data Prep

## Packages and Data

```{r}
library(tidyverse)
library(tidymodels)
library(caret)
library(yardstick)
library(ggplot2)
library(viridis)

data <- read_csv("heart.csv")

```

## 1. Run and report summary() on your data set. Then, answer the following questions:

```{r}
summary(data)
```

a.  HeartDisease is categorized as a numeric variable

b.  This does not make sense because the HeartDisease is a binary variable which only takes on 0 and 1 which is moreso categorical. This being said, it would not make sense to do any numerical summaries on this variable.

## 2. Change HeartDisease to be the appropriate data type, and name it something different. In the same tidyverse pipeline, remove the ST_Slope variable and the original HeartDisease variable. Save your new data set as new_heart. We will use this new data set for the remainder of the assignment.

```{r}
new_heart <- data |>
  mutate(factor_HeartDisease = factor(HeartDisease)) |>
  select(-ST_Slope, -HeartDisease)
```

# Task 3: EDA
## 1. We are going to model someone’s age (our response variable) as a function of heart disease and their max heart rate. First, create the appropriate scatterplot to visualize this relationship. Add a line to the scatterplot that represents if someone has or does not have heart disease. Remove the standard error bars from the lines and add appropriate labels. Also, change the color pallet to be more colorblind friendly.

```{r}
ggplot(data = new_heart, aes(x = MaxHR, y = Age, color = factor_HeartDisease)) +
  geom_point() +
  geom_smooth(method = lm, se = F) +
  theme_minimal() +
  scale_color_viridis_d()
```
## 2. Based on visual evidence, do you think an interaction model or an additive model is more appropriate? Justify your answer.

Based on the visual evidence, it would be appropriate to use an interaction model because the slopes of the lines are not parallel, thus, indicating that there is interaction between the two predictor variables. 

# Task 4: Testing and Training
```{r}
heart_split <- initial_split(new_heart, prop = 0.8)
heart_train <- training(heart_split)
heart_test <- testing(heart_split)
```

# Task 5: OLS and LASSO
## 1. Regardless of your answer in Task 3, we are going to fit an interaction model. First fit an interaction model (named ols_mlr) with age as your response, and max heart rate + heart disease as your explanatory variables using the training data set using ordinary least squares regression. Report the summary output.
```{r}
ols_mlr <- lm(Age ~ MaxHR*factor_HeartDisease, data = new_heart)

summary(ols_mlr)
```
## 2. We are going to use RMSE to evaluate this model’s predictive performance on new data. Test your model on the testing data set. Calculate the residual mean square error (RMSE) and report it below.

```{r}
predicted_ols <- predict(ols_mlr, newdata = heart_test)

rmse <- sqrt(mean((heart_test$Age - predicted_ols)^2))
rmse
```

## 3. Now, we are going to see if a model fit using LASSO has better predictive performance than with OLS.
```{r}

LASSO_recipe <- recipe(Age ~ MaxHR + factor_HeartDisease, data = heart_train) |>
  step_dummy(factor_HeartDisease) |>
  step_normalize(MaxHR) |>
  step_interact(terms = ~ MaxHR:starts_with("factor_HeartDisease"))

 LASSO_recipe
  
```
## 4. Now, set up your appropriate spec, and grid. Next, select your final model, and report the results using the tidy() function around your model name.
```{r}
heart_cv_folds <- vfold_cv(heart_train, 10)

LASSO_spec <- linear_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")

LASSO_wkf <- workflow() |>
  add_recipe(LASSO_recipe) |>
  add_model(LASSO_spec)

LASSO_grid <- LASSO_wkf |>
  tune_grid(resamples = heart_cv_folds,
            grid = grid_regular(penalty(), levels = 30)) 

lowest_rmse <- LASSO_grid |>
  select_best(metric = "rmse")
lowest_rmse

LASSO_wkf |>
  finalize_workflow(lowest_rmse)

LASSO_final <- LASSO_wkf |>
  finalize_workflow(lowest_rmse) |>
  fit(heart_train) 
  
tidy(LASSO_final)

```

## 5. Without looking at the RMSE calculations, would you expect the RMSE calculations to be roughly the same or different? Justify your answer using output from your LASSO model.

I would expect the RMSE calculations to be roughly the same because the predictors from the LASSO model remained the same as the predictors for the OLS model.

## 6. Now compare the RMSE between your OLS and LASSO model and show that the RMSE calculations were roughly the same.
```{r}
LASSO_rmse <- LASSO_wkf |>
  finalize_workflow(lowest_rmse) |>
  last_fit(heart_split) |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  mutate(model = "LASSO") |>
  select(-.config)

OLS_rmse <- sqrt(mean((heart_test$Age - predicted_ols)^2))

OLS_rmse_tbl <- tibble(
  .metric = "rmse",
  .estimator = "standard",
  .estimate = OLS_rmse,
  model = "OLS"
)

rbind(LASSO_rmse, OLS_rmse_tbl)
```

## 7. Why are the RMSE calculations roughly the same if the coefficients for each model are different?
The LASSO model attempts to simplify the model by reducing the coefficients to be smaller or near zero while the OLS model uses the coefficients fully. Even though they are each using different processes to simplify the model, they both balance out the errors to come to a similar RMSE.

# Task 6: Logistic Regression
## 1. Propose two different logistic regression models with heart disease as our response.
```{r}
LR1_rec <- recipe(factor_HeartDisease ~ RestingBP+MaxHR, data = heart_train) |>
  step_normalize(RestingBP, MaxHR) 

LR2_rec <- recipe(factor_HeartDisease ~ Sex + Age + Cholesterol + RestingBP, data = heart_train) |>
  step_dummy(Sex) |>
  step_normalize(RestingBP, Age, Cholesterol)

LR_spec <- logistic_reg() |>
 set_engine("glm")

LR1_wkf <- workflow() |>
 add_recipe(LR1_rec) |>
 add_model(LR_spec)
LR2_wkf <- workflow() |>
 add_recipe(LR2_rec) |>
 add_model(LR_spec)

LR1_fit <- LR1_wkf |>
 fit_resamples(heart_cv_folds, metrics = metric_set(accuracy, mn_log_loss))

LR2_fit <- LR2_wkf |>
 fit_resamples(heart_cv_folds, metrics = metric_set(accuracy, mn_log_loss))

rbind(LR1_fit |> collect_metrics(),
 LR2_fit |> collect_metrics()) |>
 mutate(Model = c("Model1", "Model1", "Model2", "Model2")) |>
 select(Model, everything())


```
Model 2 is shown to be the best performing model as it uses more predictors while Model 1 is much simpler and may not be able to distinguish between people with and without heart disease as easily.

## 2. Lastly, check how well your chosen model does on the test set using the confusionMatrix() function.
```{r}
LR_train_fit <- LR2_wkf |>
 fit(heart_train)

conf_mat(heart_train |> mutate(estimate = LR_train_fit |> predict(heart_train) 
                               |> pull()), 
 factor_HeartDisease,
 estimate)
```
## 3. Next, identify the values of sensitivity and specificity, and interpret them in the context of the problem

```{r}
specificity <- 181/(181+145)

sensitivity <- 328/(328+80)

rbind(specificity, sensitivity)
```
This model correctly identifies about 55.6% of patients that do not have heart disease correctly and about 80% of people that do have heart disease correctly. 
